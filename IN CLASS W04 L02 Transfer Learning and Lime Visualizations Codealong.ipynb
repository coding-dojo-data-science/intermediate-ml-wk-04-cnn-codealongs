{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6d68d8-73cd-43f1-9a6f-a1375354135f",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Transfer Learning and Lime Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb2d56-ddf9-4553-8138-df870a37d1a7",
   "metadata": {},
   "source": [
    "In this notebook we will: \n",
    "\n",
    "1. Briefly review preparing image data and creating a CNN to classify images\n",
    "2. Demonstrate transfer learning from a pretrained CNN model\n",
    "3. Explain a model prediction using an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6363e6-9f72-4325-8a44-50b07777c79f",
   "metadata": {},
   "source": [
    "# Part 1: Intro to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2d3b5-102e-4795-8c68-059218f71d67",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "We will be using the [Muffin or Chihuahua dataset](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification).  This is from Kaggle, and includes images of muffins and various dogs (Not just chihuahuas)\n",
    "\n",
    "It was originally inspired by this meme: \n",
    "\n",
    "![Muffin or Chihuahua](https://i.postimg.cc/2SXNWP7f/muffin-meme2.jpg)\n",
    "\n",
    "\n",
    "\n",
    "# The Business Problem\n",
    "\n",
    "Our task is to create a computer vision model that can ingest an image in one of these and accurately classify it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283d48b-b360-4b4f-ad22-664d9ad08dbc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f7d61-c7a9-49ab-a26a-80e40332f73d",
   "metadata": {},
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999600c-894f-49d1-9466-3d3ca75896ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.758231Z",
     "start_time": "2023-11-04T18:55:05.741191Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import visualkeras as vk\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346528d-8ae0-4fa5-a50d-3b68df092e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016302b2-d7b1-4a35-aca6-d458c158a937",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5a0df-f74e-410f-a967-4c1e3db378f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.816177Z",
     "start_time": "2023-11-04T18:55:05.784467Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history, figsize=(6,12), marker='o'):\n",
    "       \n",
    "    # Get list of metrics from history\n",
    "    metrics = [c for c in history.history if not c.startswith('val_')]\n",
    "    \n",
    "    ## Separate row for each metric\n",
    "    fig, axes = plt.subplots(nrows=len(metrics),figsize=figsize)\n",
    "    \n",
    "    # For each metric\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "    \n",
    "        # Get the axis for the current metric\n",
    "        ax = axes[i]\n",
    "    \n",
    "        # Get metric from history.history\n",
    "        metric_values = history.history[metric_name]\n",
    "        # Get epochs from history\n",
    "        epochs = history.epoch\n",
    "    \n",
    "        # Plot the training metric\n",
    "        ax.plot(epochs, metric_values, label=metric_name, marker=marker)\n",
    "    \n",
    "        ## Check if val_{metric} exists. if so, plot:\n",
    "        val_metric_name = f\"val_{metric_name}\"\n",
    "        if val_metric_name in history.history:\n",
    "            # Get validation values and plot\n",
    "            metric_values = history.history[val_metric_name]\n",
    "            ax.plot(epochs,metric_values,label=val_metric_name, marker=marker)\n",
    "    \n",
    "        # Final subplot adjustments \n",
    "        ax.legend()\n",
    "        ax.set_title(metric_name)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "\n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "\n",
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\",\n",
    "                           class_labels=None):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0],\n",
    "                                           display_labels=class_labels);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1],\n",
    "                                            display_labels=class_labels);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True, target_names=class_labels)\n",
    "        return report_dict\n",
    "\n",
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False,\n",
    "                                    class_labels=None):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data',\n",
    "                                              class_labels=class_labels)\n",
    "        \n",
    "    \n",
    "\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data',\n",
    "                                             class_labels=class_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106576df-d4a7-4df7-bada-0fb3260ea5c5",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f6bfb-c54d-4ad4-8722-3d5770fc5c73",
   "metadata": {},
   "source": [
    "Let's take a look at the files we have to use to train and validate our model.  We will start by creating lists of paths to each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228fda5-b61f-4c65-8fed-220060b6c82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.831629Z",
     "start_time": "2023-11-04T18:55:05.816592Z"
    }
   },
   "outputs": [],
   "source": [
    "# examine folders \n",
    "folder = 'Data/muffin_chihuahua/'\n",
    "\n",
    "train_folders = glob.glob(folder + 'train/*')\n",
    "test_folders = glob.glob(folder + 'test/*')\n",
    "display('training folders', train_folders)\n",
    "print()\n",
    "display('testing folders', test_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a7000-f7b4-4f0f-b8bb-1e5f6c1dfefe",
   "metadata": {},
   "source": [
    "Now let's grab the paths to the training images so we can explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63581094-fb9e-446e-8afa-f7a00c8a8d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.024588Z",
     "start_time": "2023-11-04T18:55:05.832836Z"
    }
   },
   "outputs": [],
   "source": [
    "# get image file names\n",
    "\n",
    "train_imgs = glob.glob(folder + '/train/*/*')\n",
    "train_imgs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c6520-f9ae-483a-b40e-0743d7116277",
   "metadata": {},
   "source": [
    "## Examine One Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074bfbf-30cf-4b40-9fe2-6ce02768ea2e",
   "metadata": {},
   "source": [
    "A good place to start in exploring image data is to look at a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173108e-90d0-474a-b428-5cf544d9b72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.049288Z",
     "start_time": "2023-11-04T18:55:06.025265Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and view five images from the training images\n",
    "for image in train_imgs[:5]:\n",
    "    img_loaded = load_img(image)\n",
    "    display(img_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f84b99-3656-4d3e-b81e-9fe5a15cbe21",
   "metadata": {},
   "source": [
    "Next, let's examine the shape and the values of the array representation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592e3ed-1423-49cd-90f9-7bd1f4c1ef6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.064844Z",
     "start_time": "2023-11-04T18:55:06.049288Z"
    }
   },
   "outputs": [],
   "source": [
    "## Examine shape and values of loaded image\n",
    "img_array = img_to_array(img_loaded)\n",
    "image_shape = img_array.shape\n",
    "print(image_shape)\n",
    "img_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830fa71-3828-4f01-a07e-66ae6daee8e6",
   "metadata": {},
   "source": [
    "# Data Preparation: \n",
    "\n",
    "## Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a99b5-db5e-4481-b9ae-442625369a5f",
   "metadata": {},
   "source": [
    "Now that we have a sense of what we have, let's set up our Tensorflow Dataset.  Remember, this is an ETL pipeline that will load batches of images from our disk into memory and use those to train the model.\n",
    "\n",
    "This is important because we may not have enough computer memory to hold all of the images at the same time.  Instead, we just load a few, train the model on them, remove them from working memory, and then load the next batch.\n",
    "\n",
    "Since accessing long-term computer storage is slower than accessing working memory (RAM), we will set our Dataset object to prefetch and cache the images to improve the speed at which it's able to deliver them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7e9a6-6202-4dd2-ac39-6268de4fa695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.082561Z",
     "start_time": "2023-11-04T18:55:06.067255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define some variables for later use\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "input_shape = (img_height, img_width)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac31c0-aaaf-41d9-88d6-4687dcac706d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.182063Z",
     "start_time": "2023-11-04T18:55:06.083279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Dataset object to stream images from the file folders.\n",
    "\n",
    "# Training Dataset\n",
    "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    folder + '/train',\n",
    "    shuffle=True,\n",
    "    label_mode='categorical',\n",
    "    seed=42,\n",
    "    image_size=input_shape,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Validation and testing sets\n",
    "ds_val, ds_test = tf.keras.utils.image_dataset_from_directory(\n",
    "    folder + '/test',\n",
    "    validation_split = .5, #split the testing data into validation and test sets\n",
    "    subset='both',\n",
    "    shuffle=True,\n",
    "    label_mode='categorical',\n",
    "    seed=42,\n",
    "    image_size=input_shape,\n",
    "    batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f00177-ebb1-463e-b667-6b014b4d4a44",
   "metadata": {},
   "source": [
    "## Create a dictionary of class names\n",
    "\n",
    "The Dataset object will one-hot encode the classes, so it will be challenging to determine which class is actually being predicted by the model.\n",
    "\n",
    "We can create a dictionary of class names to help us look up the string representation of the one-hot encoded class.\n",
    "\n",
    "We have to do this before we optimize the dataset in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319aca64-00ae-4aa4-bb91-878f30fb8493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.188895Z",
     "start_time": "2023-11-04T18:55:10.183131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the class names and the number of classes\n",
    "class_names = ds_train.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Create a dictionary we can use to lookup the class names.\n",
    "\n",
    "class_dict = dict(zip(range(num_classes), class_names))\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174a79a-2e81-4ac6-bd41-5829e535846f",
   "metadata": {},
   "source": [
    "### Retrieve an element to make sure the dataset object is working correctly.\n",
    "\n",
    "We will extract one batch of images and labels and display just the first one.  We will use our `class_dict` to lookup the string name of the class label, since it is returned as a one-hot encoded array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168c246-a75c-4438-a9ad-2a2a7be59863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.273682Z",
     "start_time": "2023-11-04T18:55:10.189704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get one batch of images from the training set\n",
    "example_imgs, example_labels= ds_train.take(1).get_single_element()\n",
    "img = example_imgs[0]\n",
    "label = example_labels[0]\n",
    "input_shape = img.shape\n",
    "\n",
    "## Lookup the string name of the label and display both the label and the image\n",
    "label = class_dict[np.argmax(label)]\n",
    "print(label)\n",
    "display(array_to_img(img))\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231046b-c9de-429c-b1c4-5f3cfc5240a5",
   "metadata": {},
   "source": [
    "## Optimize Datasets for performance\n",
    "\n",
    "Next we will optimize our Datasets for performance.  They will load a new batch of images into memory while the model is training on the previous batch.  They will also cache images in a way that they can be more quickly accessed.\n",
    "\n",
    "We will use AUTOTUNE to find the optimal number of batches to prefetch and set a buffer size as large as our dataset.\n",
    "\n",
    "We will also shuffle the data.  We need to set an appropriate buffer size for the shuffling because Tensorflow needs to load in all the data to be shuffled.  If the total number of training images is larger than our working memory (RAM), then we cannot buffer them all.  However, if they CAN all be loaded into memory, then we want to do that so we get the most uniform shuffling possible.\n",
    "\n",
    "Buffer size is a value that is really specific to the machine you are using.  If you get an OOM (out of memory) error, then try reducing your buffer size.\n",
    "\n",
    "In this case our images are small and they will all fit in our memory (hopefully!) so we will set the buffer size to the full size of the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62029df3-4d68-475f-9a61-7e6a6d1c54b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.294222Z",
     "start_time": "2023-11-04T18:55:10.274781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the datasets to cache and prefetch the data.  We will also have the training batches data shuffle each time they are used.\n",
    "\n",
    "# find the optimal size for caches based on available memory\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Set training data to cache, prefect, and shuffle\n",
    "ds_train = ds_train.cache().shuffle(buffer_size=len(ds_train),\n",
    "                                    seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Set validation and testing data to cache and prefetch.  There's no reason to shuffle them.\n",
    "ds_val = ds_val.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "ds_test = ds_test.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbf3df-6fe2-4081-9b52-2d366c71fdec",
   "metadata": {},
   "source": [
    "# Load Default Model from Lecture 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a8af9-3d86-4e1a-9e14-4eb6dfa95882",
   "metadata": {},
   "source": [
    "Now let's build and fit the model!  Notice how few weights there are to train in the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad4ad1-f00d-4b8c-96dd-b95947453fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.381126Z",
     "start_time": "2023-11-04T18:55:10.310687Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load previous model\n",
    "path = 'Models/default_cnn'\n",
    "default_model = tf.keras.models.load_model(path)\n",
    "default_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e504757-8646-4e0b-a879-5293f0475027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ce13e-a924-41f3-9fc2-178414507c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:56:59.443305Z",
     "start_time": "2023-11-04T18:55:10.381126Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fit the model\n",
    "\n",
    "history = default_model.fit(ds_train,\n",
    "                    validation_data=ds_val,\n",
    "                    epochs=25,\n",
    "                    callbacks=[callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c1f9a-2cc2-4fde-8f41-c6b9033cfb37",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d895c91-e459-45d7-9c63-c8d206902732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:59:28.027849Z",
     "start_time": "2023-11-04T18:56:59.444326Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_classification_network(default_model, X_train=ds_train, X_test=ds_test, history=history,\n",
    "                                           figsize=(6,4), class_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc02d1-9fa2-44b5-8b38-1d3fba4a010a",
   "metadata": {},
   "source": [
    "Our model is pretty good!  It is making some mistakes and seems to be better at identifying chihuahuas than muffins.\n",
    "\n",
    "We did notice that our model was beginning to overfit, so some regularization might help that.  \n",
    "\n",
    "The model also learned relatively slowly.  We could consider increasing the complexity of the model as well.  We could add more convolutional and pooling layers, and/or we could add more dense layers at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc443f-a7f4-4449-bc5a-1010c5cf8cbb",
   "metadata": {},
   "source": [
    "# Part 2: Transfer Learning and Lime with CNNs\n",
    "\n",
    "## Transfer Learning\n",
    "In this section we will select a pretrained model and then fine-tune it on our data.  We will see if we can get better performance than our model above.\n",
    "\n",
    "### [Available Models in Keras](https://keras.io/api/applications/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d129087-c0c8-4cea-8250-4e022b7e146f",
   "metadata": {},
   "source": [
    "| Model             | Size (MB) | Top-1 Accuracy | Top-5 Accuracy | Parameters | Depth | Time (ms) per inference step (CPU) | Time (ms) per inference step (GPU) |\n",
    "|-------------------|-----------|----------------|----------------|------------|-------|------------------------------------|------------------------------------|\n",
    "| Xception          | 88        | 79.0%          | 94.5%          | 22.9M      | 81    | 109.4                              | 8.1                                |\n",
    "| VGG16             | 528       | 71.3%          | 90.1%          | 138.4M     | 16    | 69.5                               | 4.2                                |\n",
    "| VGG19             | 549       | 71.3%          | 90.0%          | 143.7M     | 19    | 84.8                               | 4.4                                |\n",
    "| ResNet50          | 98        | 74.9%          | 92.1%          | 25.6M      | 107   | 58.2                               | 4.6                                |\n",
    "| ResNet50V2        | 98        | 76.0%          | 93.0%          | 25.6M      | 103   | 45.6                               | 4.4                                |\n",
    "| ResNet101         | 171       | 76.4%          | 92.8%          | 44.7M      | 209   | 89.6                               | 5.2                                |\n",
    "| ResNet101V2       | 171       | 77.2%          | 93.8%          | 44.7M      | 205   | 72.7                               | 5.4                                |\n",
    "| ResNet152         | 232       | 76.6%          | 93.1%          | 60.4M      | 311   | 127.4                              | 6.5                                |\n",
    "| ResNet152V2       | 232       | 78.0%          | 94.2%          | 60.4M      | 307   | 107.5                              | 6.6                                |\n",
    "| InceptionV3       | 92        | 77.9%          | 93.7%          | 23.9M      | 189   | 42.2                               | 6.9                                |\n",
    "| InceptionResNetV2 | 215       | 80.3%          | 95.3%          | 55.9M      | 449   | 130.2                              | 10.0                               |\n",
    "| MobileNet         | 16        | 70.4%          | 89.5%          | 4.3M       | 55    | 22.6                               | 3.4                                |\n",
    "| MobileNetV2       | 14        | 71.3%          | 90.1%          | 3.5M       | 105   | 25.9                               | 3.8                                |\n",
    "| DenseNet121       | 33        | 75.0%          | 92.3%          | 8.1M       | 242   | 77.1                               | 5.4                                |\n",
    "| DenseNet169       | 57        | 76.2%          | 93.2%          | 14.3M      | 338   | 96.4                               | 6.3                                |\n",
    "| DenseNet201       | 80        | 77.3%          | 93.6%          | 20.2M      | 402   | 127.2                              | 6.7                                |\n",
    "| NASNetMobile      | 23        | 74.4%          | 91.9%          | 5.3M       | 389   | 27.0                               | 6.7                                |\n",
    "| NASNetLarge       | 343       | 82.5%          | 96.0%          | 88.9M      | 533   | 344.5                              | 20.0                               |\n",
    "| EfficientNetB0    | 29        | 77.1%          | 93.3%          | 5.3M       | 132   | 46.0                               | 4.9                                |\n",
    "| EfficientNetB1    | 31        | 79.1%          | 94.4%          | 7.9M       | 186   | 60.2                               | 5.6                                |\n",
    "| EfficientNetB2    | 36        | 80.1%          | 94.9%          | 9.2M       | 186   | 80.8                               | 6.5                                |\n",
    "| EfficientNetB3    | 48        | 81.6%          | 95.7%          | 12.3M      | 210   | 140.0                              | 8.8                                |\n",
    "| EfficientNetB4    | 75        | 82.9%          | 96.4%          | 19.5M      | 258   | 308.3                              | 15.1                               |\n",
    "| EfficientNetB5    | 118       | 83.6%          | 96.7%          | 30.6M      | 312   | 579.2                              | 25.3                               |\n",
    "| EfficientNetB6    | 166       | 84.0%          | 96.8%          | 43.3M      | 360   | 958.1                              | 40.4                               |\n",
    "| EfficientNetB7    | 256       | 84.3%          | 97.0%          | 66.7M      | 438   | 1578.9                             | 61.6                               |\n",
    "| EfficientNetV2B0  | 29        | 78.7%          | 94.3%          | 7.2M       | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B1  | 34        | 79.8%          | 95.0%          | 8.2M       | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B2  | 42        | 80.5%          | 95.1%          | 10.2M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2B3  | 59        | 82.0%          | 95.8%          | 14.5M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2S   | 88        | 83.9%          | 96.7%          | 21.6M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2M   | 220       | 85.3%          | 97.4%          | 54.4M      | -     | -                                  | -                                  |\n",
    "| EfficientNetV2L   | 479       | 85.7%          | 97.5%          | 119.0M     | -     | -                                  | -                                  |\n",
    "| ConvNeXtTiny      | 109.42    | 81.3%          | -              | 28.6M      | -     | -                                  | -                                  |\n",
    "| ConvNeXtSmall     | 192.29    | 82.3%          | -              | 50.2M      | -     | -                                  | -                                  |\n",
    "| ConvNeXtBase      | 338.58    | 85.3%          | -              | 88.5M      | -     | -                                  | -                                  |\n",
    "| ConvNeXtLarge     | 755.07    | 86.3%          | -              | 197.7M     | -     | -                                  | -                                  |\n",
    "| ConvNeXtXLarge    | 1310      | 86.7%          | -              | 350.1M     | -     | -                                  | -                                  |                               | -    |nvNeXtXLarge\t1310\t86.7%\t-\t350.1M\t-\t-\t-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31c97e-06a7-490c-a298-84382bfbed2e",
   "metadata": {},
   "source": [
    "## Select a model and load it.\n",
    "\n",
    "Remember to load it:\n",
    "\n",
    "Without top layers (we are going to add our own)\n",
    "\n",
    "with pretrained weights (imagenet)\n",
    "\n",
    "And with the correct number of classes (in this case 1000)\n",
    "\n",
    "Preprocess inputs with the .preprocess_input method.  We will do this using by wrapping the function in a `layers.Lambda` layer.  This also requires adding an Input Layer to the model as well.\n",
    "\n",
    "We also need to remember to set the model to `model.trainable = False` so we don't overwrite the trained layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa0a5c-f8f9-4e26-809d-99d310d9c9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:18:00.901110Z",
     "start_time": "2023-11-04T19:18:00.533898Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load the model.\n",
    "\n",
    "\n",
    "## Freeze the loaded weights\n",
    "\n",
    "## load the preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4a905-f08e-46ec-a28c-ef7d925ae87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall -v \"pillow<=9.5.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fef471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:18:00.936934Z",
     "start_time": "2023-11-04T19:18:00.901110Z"
    }
   },
   "outputs": [],
   "source": [
    "## Examine the shape of the new model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d055d8-e736-4e48-81af-ab716d9bb1e3",
   "metadata": {},
   "source": [
    "As we build the model we will add an Input layer to convert the inputs into tensors.  Then we will add the preprocessing layer and the pretrained model.  Finally, we will add dense layers that are trainable that we can use to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc82333-a677-40d1-8e40-ce075ae5d6b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:18:01.415240Z",
     "start_time": "2023-11-04T19:18:01.405241Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_transfer_model():\n",
    "\n",
    "    ## Initialize Model\n",
    "\n",
    "    ## Add input layer\n",
    "\n",
    "    ## Add the preprocessing layer\n",
    "\n",
    "    ## Add the pretrained layers\n",
    "\n",
    "    ## Flatten the Data\n",
    "    \n",
    "    ## Add a dropout layer for regularization\n",
    "\n",
    "    ## Add the output layer\n",
    "\n",
    "        \n",
    "    ## Compile the model and return it.\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    \n",
    "    display(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906526b-4230-474e-ad3e-65442304d256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:24:32.691898Z",
     "start_time": "2023-11-04T19:18:02.255027Z"
    }
   },
   "outputs": [],
   "source": [
    "## fit the model\n",
    "\n",
    "transfer_model = build_transfer_model()\n",
    "\n",
    "\n",
    "\n",
    "history = transfer_model.fit(ds_train,\n",
    "                              validation_data=ds_val,\n",
    "                              epochs=25,\n",
    "                              callbacks=[callbacks.EarlyStopping(patience=5)]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eea975-8c5b-481c-90f1-df0811744dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:29:48.795890Z",
     "start_time": "2023-11-04T19:24:32.692506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_classification_network(transfer_model, X_train=ds_train, X_test=ds_test, history=history,\n",
    "                                           figsize=(12,8), class_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9c892",
   "metadata": {},
   "source": [
    "Often times the transfer model will perform much better than smaller models we train ourselves, but not always.  In this case we got a big improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f272f-3d68-4a40-bd83-8355a4c8b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "\n",
    "transfer_model.save('Models/transfer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076b8bb-a10d-40c7-97a5-8fa472ffdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('Models/transfer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea374a8e-221a-4a8b-af6f-bff4a467e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification_network(loaded_model, X_train=ds_train, X_test=ds_test, history=history,\n",
    "                                           figsize=(12,8), class_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387d373-88dc-4c74-8d56-1d233079e31d",
   "metadata": {},
   "source": [
    "## Explaining CNNs with Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57565c2d-e35a-44c0-8b78-2cc3345e9e41",
   "metadata": {},
   "source": [
    "In this section we will use the LimeImageExplainer to create a visual representation of how the model is classifying images.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Extract model predictions from the model.\n",
    "2. Create a LimeImageExplainer object\n",
    "3. Create an explanation object\n",
    "4. Generate an image with a mask that shows which "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1ff93-efe7-43f9-9b6e-a46641132510",
   "metadata": {},
   "source": [
    "First, we will make a model prediction and extract the predicted and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd2004-98b4-4c44-bc03-4defd1e59f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous Helper Function    \n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y has is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y has 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "def get_true_pred_labels_images(model,ds, include_images=True, convert_y_for_sklearn=False):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: \n",
    "    https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    all_images = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "        if include_images==True:\n",
    "            all_images.extend(images)\n",
    "            \n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    ## Convert to classes (or not)\n",
    "    if convert_y_for_sklearn==True:\n",
    "        # Use our previous helper function to make preds into classes\n",
    "        y_true = convert_y_to_sklearn_classes(y_true)\n",
    "        y_pred = convert_y_to_sklearn_classes(y_pred_probs)\n",
    "        \n",
    "    else: \n",
    "        y_pred = y_pred_probs\n",
    "    \n",
    "    ## IF the images should be included or not\n",
    "    if include_images==False:\n",
    "        return y_true, y_pred\n",
    "    else:\n",
    "        # Convert images to array and return everything\n",
    "        all_images = np.array(all_images)\n",
    "        return y_true, y_pred, all_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d53a6-289f-4fa4-8045-89106b5aeede",
   "metadata": {},
   "source": [
    "Next, we will instantiate the the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f61cb1-bbaa-4d48-b318-383edd8d8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions from the model and convert to integers\n",
    "y_test, y_hat, X_test = get_true_pred_labels_images(transfer_model, ds_test, convert_y_for_sklearn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd42bf9-97fa-4fc3-acee-380f123205d2",
   "metadata": {},
   "source": [
    "Let's examine an image and its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb784d5-8810-49db-868c-5b69ddbf5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select an image to explain\n",
    "i = ##\n",
    "display(array_to_img(X_test[i]))\n",
    "\n",
    "## Get the label from the class_dict dictionary\n",
    "print('True label', class_dict[y_test[i]])\n",
    "print('predicted label', class_dict[y_hat[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65861a11-2929-4f96-827f-a8be699c8a3b",
   "metadata": {},
   "source": [
    "Now we will create a Lime explanation for this model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd14a7b-c1bd-434c-a9f4-ae47e2d306e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image\n",
    "# !pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cab6ae-5da7-4a20-81d2-8df5b0ecf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23fc1dc-4eae-43d5-b133-a16e11e62264",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Explainer\n",
    "explainer = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022123-3b61-48a6-8088-6c6693d20cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create and explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809780f-ce27-4a48-81e0-3c673c92a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a mask to show the colors for each class\n",
    "\n",
    "## Mark the boundaries and show the image combined with the mask\n",
    "plt.title(f'Segments that Pushed Prediction Towards (Green) or Away (Red) from {label}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acccee-2a15-4560-96da-23c19b8603a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f1380-30b2-4787-9d89-d9640eb2f5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
