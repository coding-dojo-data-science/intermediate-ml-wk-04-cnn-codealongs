{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6d68d8-73cd-43f1-9a6f-a1375354135f",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Transfer Learning and Lime Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb2d56-ddf9-4553-8138-df870a37d1a7",
   "metadata": {},
   "source": [
    "In this notebook we will: \n",
    "\n",
    "1. Briefly review preparing image data and creating a CNN to classify images\n",
    "2. Demonstrate transfer learning from a pretrained CNN model\n",
    "3. Explain a model prediction using an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6363e6-9f72-4325-8a44-50b07777c79f",
   "metadata": {},
   "source": [
    "# Part 1: Intro to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2d3b5-102e-4795-8c68-059218f71d67",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "We will be using the [Muffin or Chihuahua dataset](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification).  This is from Kaggle, and includes images of muffins and various dogs (Not just chihuahuas)\n",
    "\n",
    "It was originally inspired by this meme: \n",
    "\n",
    "![Muffin or Chihuahua](https://i.postimg.cc/2SXNWP7f/muffin-meme2.jpg)\n",
    "\n",
    "\n",
    "\n",
    "# The Business Problem\n",
    "\n",
    "Our task is to create a computer vision model that can ingest an image in one of these and accurately classify it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283d48b-b360-4b4f-ad22-664d9ad08dbc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999600c-894f-49d1-9466-3d3ca75896ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.758231Z",
     "start_time": "2023-11-04T18:55:05.741191Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "## New Keras image tools\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import visualkeras as vk\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346528d-8ae0-4fa5-a50d-3b68df092e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for GPU availability\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aaad3e-3e5f-4341-8f2b-9b929ef3d91a",
   "metadata": {},
   "source": [
    "### How to install tensorflow in a new environment so it supports GPUs\n",
    "[Install Tensorflow with Pip](https://www.tensorflow.org/install/pip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016302b2-d7b1-4a35-aca6-d458c158a937",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5a0df-f74e-410f-a967-4c1e3db378f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.816177Z",
     "start_time": "2023-11-04T18:55:05.784467Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history, figsize=(6,12), marker='o'):\n",
    "       \n",
    "    # Get list of metrics from history\n",
    "    metrics = [c for c in history.history if not c.startswith('val_')]\n",
    "    \n",
    "    ## Separate row for each metric\n",
    "    fig, axes = plt.subplots(nrows=len(metrics),figsize=figsize)\n",
    "    \n",
    "    # For each metric\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "    \n",
    "        # Get the axis for the current metric\n",
    "        ax = axes[i]\n",
    "    \n",
    "        # Get metric from history.history\n",
    "        metric_values = history.history[metric_name]\n",
    "        # Get epochs from history\n",
    "        epochs = history.epoch\n",
    "    \n",
    "        # Plot the training metric\n",
    "        ax.plot(epochs, metric_values, label=metric_name, marker=marker)\n",
    "    \n",
    "        ## Check if val_{metric} exists. if so, plot:\n",
    "        val_metric_name = f\"val_{metric_name}\"\n",
    "        if val_metric_name in history.history:\n",
    "            # Get validation values and plot\n",
    "            metric_values = history.history[val_metric_name]\n",
    "            ax.plot(epochs,metric_values,label=val_metric_name, marker=marker)\n",
    "    \n",
    "        # Final subplot adjustments \n",
    "        ax.legend()\n",
    "        ax.set_title(metric_name)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "\n",
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "\n",
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\",\n",
    "                           class_labels=None):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0],\n",
    "                                           display_labels=class_labels);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1],\n",
    "                                            display_labels=class_labels);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True, target_names=class_labels)\n",
    "        return report_dict\n",
    "\n",
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False,\n",
    "                                    class_labels=None):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data',\n",
    "                                              class_labels=class_labels)\n",
    "        \n",
    "    \n",
    "\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data',\n",
    "                                             class_labels=class_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106576df-d4a7-4df7-bada-0fb3260ea5c5",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f6bfb-c54d-4ad4-8722-3d5770fc5c73",
   "metadata": {},
   "source": [
    "Let's take a look at the files we have to use to train and validate our model.  We will start by creating lists of paths to each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228fda5-b61f-4c65-8fed-220060b6c82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:05.831629Z",
     "start_time": "2023-11-04T18:55:05.816592Z"
    }
   },
   "outputs": [],
   "source": [
    "# examine folders \n",
    "folder = 'Data/muffin_chihuahua/'\n",
    "\n",
    "train_folders = glob.glob(folder + 'train/*')\n",
    "test_folders = glob.glob(folder + 'test/*')\n",
    "display('training folders', train_folders)\n",
    "print()\n",
    "display('testing folders', test_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a7000-f7b4-4f0f-b8bb-1e5f6c1dfefe",
   "metadata": {},
   "source": [
    "Now let's grab the paths to the training images so we can explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63581094-fb9e-446e-8afa-f7a00c8a8d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.024588Z",
     "start_time": "2023-11-04T18:55:05.832836Z"
    }
   },
   "outputs": [],
   "source": [
    "# get image file names\n",
    "\n",
    "train_imgs = glob.glob(folder + '/train/*/*')\n",
    "train_imgs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c6520-f9ae-483a-b40e-0743d7116277",
   "metadata": {},
   "source": [
    "## Examine Sample Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074bfbf-30cf-4b40-9fe2-6ce02768ea2e",
   "metadata": {},
   "source": [
    "A good place to start in exploring image data is to look at a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173108e-90d0-474a-b428-5cf544d9b72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.049288Z",
     "start_time": "2023-11-04T18:55:06.025265Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and view five images from the training images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f84b99-3656-4d3e-b81e-9fe5a15cbe21",
   "metadata": {},
   "source": [
    "Next, let's examine the shape and the values of the array representation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592e3ed-1423-49cd-90f9-7bd1f4c1ef6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.064844Z",
     "start_time": "2023-11-04T18:55:06.049288Z"
    }
   },
   "outputs": [],
   "source": [
    "## Examine shape and values of loaded image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830fa71-3828-4f01-a07e-66ae6daee8e6",
   "metadata": {},
   "source": [
    "# Data Preparation: \n",
    "\n",
    "## Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a99b5-db5e-4481-b9ae-442625369a5f",
   "metadata": {},
   "source": [
    "Now that we have a sense of what we have, let's set up our Tensorflow Dataset.  Remember, this is an ETL pipeline that will load batches of images from our disk into memory and use those to train the model.\n",
    "\n",
    "This is important because we may not have enough computer memory to hold all of the images at the same time.  Instead, we just load a few, train the model on them, remove them from working memory, and then load the next batch.\n",
    "\n",
    "Since accessing long-term computer storage is slower than accessing working memory (RAM), we will set our Dataset object to prefetch and cache the images to improve the speed at which it's able to deliver them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7e9a6-6202-4dd2-ac39-6268de4fa695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:06.082561Z",
     "start_time": "2023-11-04T18:55:06.067255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define some variables for later use\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "input_shape = (img_height, img_width)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac31c0-aaaf-41d9-88d6-4687dcac706d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.182063Z",
     "start_time": "2023-11-04T18:55:06.083279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Dataset object to stream images from the file folders.\n",
    "\n",
    "# Training Dataset\n",
    "\n",
    "\n",
    "# Validation and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f00177-ebb1-463e-b667-6b014b4d4a44",
   "metadata": {},
   "source": [
    "## Create a dictionary of class names\n",
    "\n",
    "The Dataset object will one-hot encode the classes, so it will be challenging to determine which class is actually being predicted by the model.\n",
    "\n",
    "We can create a dictionary of class names to help us look up the string representation of the one-hot encoded class.\n",
    "\n",
    "We have to do this before we optimize the dataset in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319aca64-00ae-4aa4-bb91-878f30fb8493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.188895Z",
     "start_time": "2023-11-04T18:55:10.183131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the class names and the number of classes\n",
    "\n",
    "\n",
    "# Create a dictionary we can use to lookup the class names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174a79a-2e81-4ac6-bd41-5829e535846f",
   "metadata": {},
   "source": [
    "### Retrieve an element to make sure the dataset object is working correctly.\n",
    "\n",
    "We will extract one batch of images and labels and display just the first one.  We will use our `class_dict` to lookup the string name of the class label, since it is returned as a one-hot encoded array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168c246-a75c-4438-a9ad-2a2a7be59863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.273682Z",
     "start_time": "2023-11-04T18:55:10.189704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get one batch of images from the training set\n",
    "example_imgs, example_labels= ds_train.take(1).get_single_element()\n",
    "img = example_imgs[0]\n",
    "label = example_labels[0]\n",
    "input_shape = img.shape\n",
    "\n",
    "## Lookup the string name of the label and display both the label and the image\n",
    "label = class_dict[np.argmax(label)]\n",
    "print(label)\n",
    "display(array_to_img(img))\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231046b-c9de-429c-b1c4-5f3cfc5240a5",
   "metadata": {},
   "source": [
    "## Optimize Datasets for performance\n",
    "\n",
    "Next we will optimize our Datasets for performance.  They will load a new batch of images into memory while the model is training on the previous batch.  They will also cache images in a way that they can be more quickly accessed.\n",
    "\n",
    "We will use AUTOTUNE to find the optimal number of batches to prefetch and set a buffer size as large as our dataset.\n",
    "\n",
    "We will also shuffle the data.  We need to set an appropriate buffer size for the shuffling because Tensorflow needs to load in all the data to be shuffled.  If the total number of training images is larger than our working memory (RAM), then we cannot buffer them all.  However, if they CAN all be loaded into memory, then we want to do that so we get the most uniform shuffling possible.\n",
    "\n",
    "Buffer size is a value that is really specific to the machine you are using.  If you get an OOM (out of memory) error, then try reducing your buffer size.\n",
    "\n",
    "In this case our images are small and they will all fit in our memory (hopefully!) so we will set the buffer size to the full size of the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62029df3-4d68-475f-9a61-7e6a6d1c54b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.294222Z",
     "start_time": "2023-11-04T18:55:10.274781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the datasets to cache and prefetch the data.  We will also have the training batches data shuffle each time they are used.\n",
    "\n",
    "# find the optimal size for caches based on available memory\n",
    "\n",
    "\n",
    "# Set training data to cache, prefect, and shuffle\n",
    "\n",
    "\n",
    "# Set validation and testing data to cache and prefetch.  There's no reason to shuffle them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbf3df-6fe2-4081-9b52-2d366c71fdec",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We are ready to model the data.  \n",
    "\n",
    "We will start with a rescaling layer to adjust the image values from a range of 0 to 255 to a range of 0 to 1.  This helps speed up learning for our model.\n",
    "\n",
    "Then we will add convolutional layer and a pooling layer.\n",
    "\n",
    "After those we will flatten the data with a flatten layer to prepare it for the dense output layer.\n",
    "\n",
    "The output layer will have the same number of units as the number of classes and an softmax activation function.\n",
    "\n",
    "This will be a very simple model, and is unlikely to fit the data very well.\n",
    "\n",
    "It is best practice to create a function to generate the model.  This practice is good in case we want to tune it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b90f1-192a-444d-aeef-d5ff6a2a75ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.309866Z",
     "start_time": "2023-11-04T18:55:10.294222Z"
    }
   },
   "outputs": [],
   "source": [
    "## Default model building function\n",
    "def build_default_model():\n",
    "    # Build the model\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    ## Rescaling Layer\n",
    "\n",
    "    ## Convolutional Layer\n",
    "\n",
    "    ## Pooling Layer\n",
    "\n",
    "    ## Flatten Layer\n",
    "\n",
    "    ## Dense Layer\n",
    "\n",
    "    ## Output Layer\n",
    "\n",
    "\n",
    "    ## Compile Model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a8af9-3d86-4e1a-9e14-4eb6dfa95882",
   "metadata": {},
   "source": [
    "Now let's build and fit the model!  Notice how few weights there are to train in the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad4ad1-f00d-4b8c-96dd-b95947453fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:55:10.381126Z",
     "start_time": "2023-11-04T18:55:10.310687Z"
    }
   },
   "outputs": [],
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ce13e-a924-41f3-9fc2-178414507c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:56:59.443305Z",
     "start_time": "2023-11-04T18:55:10.381126Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c1f9a-2cc2-4fde-8f41-c6b9033cfb37",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d895c91-e459-45d7-9c63-c8d206902732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T18:59:28.027849Z",
     "start_time": "2023-11-04T18:56:59.444326Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate the model:  X_train, X_test = dataset objects\n",
    "evaluate_classification_network(default_model, X_train=ds_train, X_test=ds_test, history=history,\n",
    "                                           figsize=(6,4), class_labels=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc02d1-9fa2-44b5-8b38-1d3fba4a010a",
   "metadata": {},
   "source": [
    "Our model is pretty good!  It is making some mistakes and seems to be better at identifying chihuahuas than muffins.\n",
    "\n",
    "We did notice that our model was beginning to overfit, so some regularization might help that.  \n",
    "\n",
    "The model also learned relatively slowly.  We could consider increasing the complexity of the model as well.  We could add more convolutional and pooling layers, and/or we could add more dense layers at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9711ac-2a1e-44c1-9d1d-aae6db36b496",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save this model so we don't have to retrain it every time we run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726bbf2-f89e-47ba-9840-541b04d5623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "path = 'Models/default_cnn'\n",
    "# default_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c76b2-679b-4943-b606-a68a44cfe41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model to ensure it saved correctly\n",
    "loaded_model = tf.keras.models.load_model(path)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc599475-bece-4f43-9db8-98ebb2c1044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model again to ensure it's working properly\n",
    "evaluate_classification_network(loaded_model, X_train=ds_train, X_test=ds_test, history=history,\n",
    "                                           figsize=(6,4), class_labels=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
